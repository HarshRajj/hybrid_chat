# üåç Vietnam Travel Assistant# üåç Vietnam Travel Assistant - Hybrid RAG System# üåç Vietnam Travel Assistant - Hybrid RAG System



A smart travel assistant powered by **Hybrid RAG** (Retrieval-Augmented Generation). Combines semantic search (Pinecone), graph traversal (Neo4j), and GPT-4 to help users plan trips to Vietnam.



**Key Features:** Intelligent routing, async I/O, multi-level caching, conversation history, streaming responses.An intelligent travel assistant for Vietnam using **Hybrid RAG** (Retrieval-Augmented Generation) that combines vector search (Pinecone) and graph traversal (Neo4j) with LLM-powered responses.An intelligent travel assistant for Vietnam using **Hybrid RAG** (Retrieval-Augmented Generation) that combines vector search (Pinecone) and graph traversal (Neo4j) with smart query understanding.



---



## üöÄ Quick Start## ‚ö° Quick Start## ‚ö° Quick Start



### 1. Clone & Install

```bash

git clone https://github.com/HarshRajj/hybrid_chat.git### 1. Install Dependencies### 1. Install Dependencies

cd hybrid_chat

pip install -r requirements.txt```bash```bash

```

pip install -r requirements.txtpip install -r requirements.txt

### 2. Configure Environment

Create a `.env` file:``````

```env

OPENAI_API_KEY=sk-your-key-here

PINECONE_API_KEY=your-pinecone-key

NEO4J_URI=bolt://localhost:7687### 2. Set Up Environment### 2. Set Up Environment

NEO4J_USER=neo4j

NEO4J_PASSWORD=your-passwordCreate a `.env` file in the project root:Create a `.env` file in the project root:

```

```env```env

### 3. Load Data

```bashOPENAI_API_KEY=sk-your-key-hereOPENAI_API_KEY=sk-your-key-here

python scripts/load_to_neo4j.py        # Load 360 nodes to Neo4j

python scripts/pinecone_upload_scaled.py  # Upload vectors to PineconePINECONE_API_KEY=your-pinecone-keyPINECONE_API_KEY=your-pinecone-key

```

NEO4J_URI=bolt://localhost:7687NEO4J_URI=bolt://localhost:7687

### 4. Run

```bashNEO4J_USER=neo4jNEO4J_USER=neo4j

python main.py          # Normal mode

python main.py --stream # Streaming mode (real-time output)NEO4J_PASSWORD=your-passwordNEO4J_PASSWORD=your-password

```

``````

---



## üí¨ Usage

### 3. Load Data### 3. Load Data

**Try these queries:**

``````bash```bash

"best hotels in Hanoi"

"4 day itinerary for Vietnam" # Load travel data to Neo4j# Load travel data to Neo4j

"restaurants near Hoan Kiem Lake"

"hi" ‚Üí instant greeting!python scripts/load_to_neo4j.pypython scripts/load_to_neo4j.py

```



**Commands:**

- `stats` - View performance metrics# Upload vectors to Pinecone# Upload vectors to Pinecone

- `cache` - View cache statistics

- `exit` - Exit with summarypython scripts/pinecone_upload_scaled.pypython scripts/pinecone_upload_scaled.py



---``````



## ‚ö° What Makes It Fast



- **Async I/O**: Vector + graph queries run in parallel (40-60% faster)### 4. Run the Assistant### 4. Run the Assistant

- **5-Layer Caching**: Embeddings, vector DB, graph DB, LLM responses, understanding

- **Smart Routing**: Auto-selects hybrid/graph/direct strategies```bash```bash

- **Instant Greetings**: No database calls for simple queries

# Normal modepython main.py

**Performance:**

- 95% faster for repeat queries (caching)python main.py```

- <0.001s for greetings

- 85-95% cache hit rate



---# Streaming mode (real-time output)## üéØ What It Does



## üìÅ Project Structurepython main.py --stream



``````- **Understands queries intelligently** - Only asks clarification when truly needed

hybrid_chat/

‚îú‚îÄ‚îÄ src/- **Searches semantically** - Finds relevant hotels, attractions, restaurants using Pinecone

‚îÇ   ‚îú‚îÄ‚îÄ agent.py              # Main orchestrator (430 lines, async)

‚îÇ   ‚îú‚îÄ‚îÄ router.py             # Query routing logic---## üéØ What It Does

‚îÇ   ‚îú‚îÄ‚îÄ prompt.py             # System prompts

‚îÇ   ‚îú‚îÄ‚îÄ embeddings/embedder.py   # OpenAI embeddings

‚îÇ   ‚îú‚îÄ‚îÄ vector/pinecone_client.py  # Pinecone wrapper

‚îÇ   ‚îî‚îÄ‚îÄ graph/neo4j_client.py      # Neo4j wrapper## üéØ Features- **Understands queries intelligently** - Only asks clarification when truly needed

‚îú‚îÄ‚îÄ data/vietnam_travel_dataset.json  # 360 travel nodes

‚îú‚îÄ‚îÄ scripts/                  # Data loading scripts- **Searches semantically** - Finds relevant hotels, attractions, restaurants using Pinecone

‚îî‚îÄ‚îÄ main.py                   # Interactive CLI

```### Core Capabilities- **Explores relationships** - Discovers nearby places and connections using Neo4j



---- ‚úÖ **Smart Query Routing** - Automatically selects hybrid/graph/direct strategies- **Generates smart responses** - Combines all context with GPT-4 for helpful answers



## üîß Tech Stack- ‚úÖ **Semantic Search** - Finds relevant hotels, attractions, restaurants via Pinecone- **Lightning fast** - Multi-level caching + streaming responses



- Python 3.11 + asyncio- ‚úÖ **Graph Traversal** - Discovers nearby places and connections via Neo4j

- OpenAI (text-embedding-3-small + gpt-4o-mini)

- Pinecone (vector DB)- ‚úÖ **Conversational** - Handles greetings and maintains conversation history## ‚ö° Performance Features

- Neo4j (graph DB)

- ‚úÖ **Out-of-Scope Detection** - Politely refuses non-Vietnam queries

---

- **Async/Await Architecture**: True concurrent I/O for maximum speed (40-60% faster!)

## üìö Learn More

### Performance Optimizations- **Response Caching**: Instant answers for repeat queries (95% faster!)

- **[IMPROVEMENTS.md](IMPROVEMENTS.md)** - All improvements over the original version

- **[hybrid_chat.py](hybrid_chat.py)** - Original monolithic version (reference)- ‚ö° **Async/Await** - True concurrent I/O with `AsyncOpenAI` + `asyncio` (40-60% faster)- **Dynamic Token Sizing**: Optimizes LLM cost based on query complexity



---- üíæ **Multi-Level Caching** - Embeddings, vector results, graph queries, and LLM responses- **Streaming Output**: Real-time response generation for better UX



**Built by [HarshRajj](https://github.com/HarshRajj)** ‚Ä¢ Hybrid RAG ‚Ä¢ Async Python ‚Ä¢ Production-Ready- üìä **Response Time Tracking** - Detailed breakdown of Phase 1, routing, embedding, vector/graph search, and LLM generation- **Multi-level Caching**: Embeddings, vector search, graph queries all cached


- üéØ **Dynamic Token Sizing** - Adjusts `max_tokens` based on query strategy (400/500/600)- **Parallel Processing**: Concurrent vector + graph fetching with asyncio

- üîÑ **Streaming Responses** - Real-time output for better UX

- üîÅ **Retry Logic** - Automatic retry on API failures (tenacity)See [ASYNC_ARCHITECTURE.md](ASYNC_ARCHITECTURE.md) for detailed async implementation.



---## üìä Example Queries



## üìä Example Queries```

‚úÖ "4 day romantic itinerary for Vietnam"

```‚úÖ "best beach hotels in Danang"

User: "hi"‚úÖ "hotels in Vietnam"

‚Üí Instant greeting (no database calls)‚úÖ "restaurants near Hoan Kiem Lake"

‚úÖ "what to see in Hue"

User: "4 day romantic itinerary for Vietnam"```

‚Üí Creates detailed itinerary for popular romantic destinations

## üèóÔ∏è Architecture

User: "best beach hotels in Danang"

‚Üí Searches Pinecone + Neo4j ‚Üí Lists top beach hotels with details```

User Query

User: "hotels near Hoan Kiem Lake"    ‚Üì

‚Üí Graph-focused search ‚Üí Finds hotels using proximity relationshipsPhase 1: Understanding (LLM analyzes query)

    ‚Üì

User: "trip to Paris"Phase 2: Routing (graph/vector/hybrid/direct)

‚Üí Out-of-scope detection ‚Üí Politely refuses, suggests Vietnam instead    ‚Üì

```Retrieval (Pinecone + Neo4j)

    ‚Üì

---Response Generation (GPT-4)

```

## üèóÔ∏è Architecture

## üìÅ Project Structure

```

User Query```

    ‚Üìhybrid_chat/

Phase 1: Quick Checks‚îú‚îÄ‚îÄ src/

  - Greeting detection (instant response)‚îÇ   ‚îú‚îÄ‚îÄ agent.py           # Main orchestrator (two-phase system)

  - Out-of-scope detection (refuses politely)‚îÇ   ‚îú‚îÄ‚îÄ router.py          # Intelligent query routing

    ‚Üì‚îÇ   ‚îú‚îÄ‚îÄ prompt.py          # System prompts

Phase 2: Smart Routing‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Environment configuration

  - Rule-based: "nearby" ‚Üí graph, "weather" ‚Üí direct‚îÇ   ‚îú‚îÄ‚îÄ embeddings/        # OpenAI embeddings with caching

  - Default: hybrid strategy‚îÇ   ‚îú‚îÄ‚îÄ vector/            # Pinecone client

    ‚Üì‚îÇ   ‚îî‚îÄ‚îÄ graph/             # Neo4j client

Async Retrieval (Parallel Execution)‚îú‚îÄ‚îÄ data/

  - Vector Search (Pinecone) + Graph Search (Neo4j)‚îÇ   ‚îî‚îÄ‚îÄ vietnam_travel_dataset.json

  - Runs concurrently via asyncio‚îú‚îÄ‚îÄ scripts/

    ‚Üì‚îÇ   ‚îú‚îÄ‚îÄ load_to_neo4j.py

Response Generation (AsyncOpenAI)‚îÇ   ‚îî‚îÄ‚îÄ pinecone_upload_scaled.py

  - Streaming or normal mode‚îú‚îÄ‚îÄ main.py                # Interactive CLI

  - Cached for repeat queries‚îú‚îÄ‚îÄ requirements.txt

```‚îî‚îÄ‚îÄ .env                   # Your API keys (create this)

```

---

## ÔøΩ Usage Modes

## üìÅ Project Structure

### Normal Mode

``````bash

hybrid_chat/python main.py

‚îú‚îÄ‚îÄ src/```

‚îÇ   ‚îú‚îÄ‚îÄ agent.py           # Main orchestrator (430 lines, async)

‚îÇ   ‚îú‚îÄ‚îÄ router.py          # Smart routing logic### Streaming Mode (Real-time Output)

‚îÇ   ‚îú‚îÄ‚îÄ prompt.py          # Centralized system prompts```bash

‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Environment variablespython main.py --stream

‚îÇ   ‚îú‚îÄ‚îÄ embeddings/```

‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embedder.py    # OpenAI embeddings with cache

‚îÇ   ‚îú‚îÄ‚îÄ vector/### Interactive Commands

‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pinecone_client.py  # Pinecone wrapper with cache- Type your question to get answers

‚îÇ   ‚îî‚îÄ‚îÄ graph/- `stats` - View response time statistics

‚îÇ       ‚îî‚îÄ‚îÄ neo4j_client.py     # Neo4j wrapper with parallel queries- `cache` - View cache performance

‚îú‚îÄ‚îÄ data/- `exit` - Exit (shows summary)

‚îÇ   ‚îî‚îÄ‚îÄ vietnam_travel_dataset.json  # 360 nodes

‚îú‚îÄ‚îÄ scripts/## ÔøΩüîß Technologies

‚îÇ   ‚îú‚îÄ‚îÄ load_to_neo4j.py           # Load graph data

‚îÇ   ‚îî‚îÄ‚îÄ pinecone_upload_scaled.py  # Upload vectors- **OpenAI**: text-embedding-3-small (embeddings) + gpt-4o-mini (chat)

‚îú‚îÄ‚îÄ main.py                # Interactive CLI- **Pinecone**: Serverless vector database with caching

‚îú‚îÄ‚îÄ requirements.txt- **Neo4j**: Graph database for relationships with parallel queries

‚îú‚îÄ‚îÄ .env                   # Your API keys (create this!)- **Python 3.x**: tenacity, tqdm, asyncio, concurrent.futures

‚îú‚îÄ‚îÄ README.md              # This file

‚îî‚îÄ‚îÄ IMPROVEMENTS.md        # All improvements over hybrid_chat.py## üìö Documentation

```

- **IMPROVEMENTS.md** - Complete list of enhancements from original `hybrid_chat.py`

---- **LLM_OPTIMIZATION.md** - Details on response caching, dynamic tokens, and streaming

- **TIMING_TRACKING.md** - Response time tracking and performance monitoring

## üí¨ Usage- **PHASE1_SIMPLIFICATION.md** - Query understanding approach

- **hybrid_chat.py** - Original version for reference

### Interactive Mode

```bash## üß™ Testing

python main.py

``````bash

# Test LLM optimizations (caching, tokens, streaming)

**Commands:**python test_llm_optimization.py

- Type any travel question to get answers

- `stats` - View response time statistics# Test timing tracking

- `cache` - View cache performancepython test_timing.py

- `exit` - Exit with summary

# Test simplified Phase 1

### Streaming Modepython test_simplified_phase1.py

```bash```

python main.py --stream

```## üßπ Cleanup (Optional)

Shows responses word-by-word as they generate (feels instant!)

To remove test files and redundant documentation:

---```bash

# Run the cleanup script

## üîß Technologies.\cleanup.ps1

```

- **Python 3.11+** with asyncio

- **OpenAI** - `text-embedding-3-small` + `gpt-4o-mini` (AsyncOpenAI)This keeps only essential files: README, IMPROVEMENTS, source code, and data.

- **Pinecone** - Serverless vector database (360 embeddings, cached)

- **Neo4j** - Graph database (360 nodes, parallel batch queries)## ü§ù Support

- **Libraries** - `aiohttp`, `tenacity` (retry), `tqdm` (progress bars)

This is an internship assignment project demonstrating hybrid RAG architecture with intelligent query understanding.

---

## üìà Performance Stats

| Metric | Before (hybrid_chat.py) | After | Improvement |
|--------|-------------------------|-------|-------------|
| **Code Size** | 1 monolithic file | Modular (7 files) | Clean architecture |
| **Query Speed** | Synchronous | Async (parallel I/O) | **40-60% faster** |
| **Repeat Queries** | 6-10s | 0.3-0.5s (cached) | **95% faster** |
| **Greeting Handling** | 6-10s | < 0.001s | **Instant** |
| **Cache Hit Rate** | None | 85-95% | Huge savings |

---

## üìö Documentation

- **IMPROVEMENTS.md** - Complete list of improvements from hybrid_chat.py
- **hybrid_chat.py** - Original version (kept for reference)

---

## üé§ Interview Highlights

**"Walk me through your architecture":**
> "The system has 430 lines in `agent.py` with 2 phases:
> 
> **Phase 1**: Quick checks (greetings, out-of-scope) - instant response
> 
> **Phase 2**: Smart routing ‚Üí 3 strategies (hybrid/graph/direct)
> 
> Key optimizations:
> - **Async I/O**: Vector + graph queries run in parallel via `asyncio`
> - **Unified cache**: 3 layers (understanding, responses, timings)
> - **Helper methods**: `_generate_llm_response()` eliminates duplication
> 
> Performance: 95% faster for repeat queries, 40-60% faster overall!"

**"What's special about your caching?":**
> "Multi-level caching at every I/O point:
> 1. Embedding cache (in `Embedder`)
> 2. Vector query cache (in `PineconeClient`)
> 3. Graph query cache (in `Neo4jClient`)
> 4. LLM response cache (in `HybridRunner`)
> 
> This gives 85-95% cache hit rate for common queries!"

**"How do you handle async?":**
> "I use `AsyncOpenAI` for native async LLM calls, and wrap synchronous Pinecone/Neo4j calls with `run_in_executor()` to run them in thread pools. This allows parallel execution without blocking. This is optimal because Pinecone and Neo4j don't have official async SDKs."

---

## ü§ù About

This is a demonstration project showcasing hybrid RAG architecture with intelligent query routing, async I/O optimization, and multi-level caching for a Vietnam travel assistant chatbot.

**Built with:** Python, OpenAI, Pinecone, Neo4j, asyncio
