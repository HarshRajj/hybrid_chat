# ğŸŒ Vietnam Travel Assistant - Hybrid RAG System

An intelligent travel assistant for Vietnam using **Hybrid RAG** (Retrieval-Augmented Generation) that combines vector search (Pinecone) and graph traversal (Neo4j) with smart query understanding.

## âš¡ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Set Up Environment
Create a `.env` file in the project root:
```env
OPENAI_API_KEY=sk-your-key-here
PINECONE_API_KEY=your-pinecone-key
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your-password
```

### 3. Load Data
```bash
# Load travel data to Neo4j
python scripts/load_to_neo4j.py

# Upload vectors to Pinecone
python scripts/pinecone_upload_scaled.py
```

### 4. Run the Assistant
```bash
python main.py
```

## ğŸ¯ What It Does

- **Understands queries intelligently** - Only asks clarification when truly needed
- **Searches semantically** - Finds relevant hotels, attractions, restaurants using Pinecone
## ğŸ¯ What It Does

- **Understands queries intelligently** - Only asks clarification when truly needed
- **Searches semantically** - Finds relevant hotels, attractions, restaurants using Pinecone
- **Explores relationships** - Discovers nearby places and connections using Neo4j
- **Generates smart responses** - Combines all context with GPT-4 for helpful answers
- **Lightning fast** - Multi-level caching + streaming responses

## âš¡ Performance Features

- **Async/Await Architecture**: True concurrent I/O for maximum speed (40-60% faster!)
- **Response Caching**: Instant answers for repeat queries (95% faster!)
- **Dynamic Token Sizing**: Optimizes LLM cost based on query complexity
- **Streaming Output**: Real-time response generation for better UX
- **Multi-level Caching**: Embeddings, vector search, graph queries all cached
- **Parallel Processing**: Concurrent vector + graph fetching with asyncio

See [ASYNC_ARCHITECTURE.md](ASYNC_ARCHITECTURE.md) for detailed async implementation.

## ğŸ“Š Example Queries

```
âœ… "4 day romantic itinerary for Vietnam"
âœ… "best beach hotels in Danang"
âœ… "hotels in Vietnam"
âœ… "restaurants near Hoan Kiem Lake"
âœ… "what to see in Hue"
```

## ğŸ—ï¸ Architecture

```
User Query
    â†“
Phase 1: Understanding (LLM analyzes query)
    â†“
Phase 2: Routing (graph/vector/hybrid/direct)
    â†“
Retrieval (Pinecone + Neo4j)
    â†“
Response Generation (GPT-4)
```

## ğŸ“ Project Structure

```
hybrid_chat/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent.py           # Main orchestrator (two-phase system)
â”‚   â”œâ”€â”€ router.py          # Intelligent query routing
â”‚   â”œâ”€â”€ prompt.py          # System prompts
â”‚   â”œâ”€â”€ config.py          # Environment configuration
â”‚   â”œâ”€â”€ embeddings/        # OpenAI embeddings with caching
â”‚   â”œâ”€â”€ vector/            # Pinecone client
â”‚   â””â”€â”€ graph/             # Neo4j client
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vietnam_travel_dataset.json
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ load_to_neo4j.py
â”‚   â””â”€â”€ pinecone_upload_scaled.py
â”œâ”€â”€ main.py                # Interactive CLI
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env                   # Your API keys (create this)
```

## ï¿½ Usage Modes

### Normal Mode
```bash
python main.py
```

### Streaming Mode (Real-time Output)
```bash
python main.py --stream
```

### Interactive Commands
- Type your question to get answers
- `stats` - View response time statistics
- `cache` - View cache performance
- `exit` - Exit (shows summary)

## ï¿½ğŸ”§ Technologies

- **OpenAI**: text-embedding-3-small (embeddings) + gpt-4o-mini (chat)
- **Pinecone**: Serverless vector database with caching
- **Neo4j**: Graph database for relationships with parallel queries
- **Python 3.x**: tenacity, tqdm, asyncio, concurrent.futures

## ğŸ“š Documentation

- **IMPROVEMENTS.md** - Complete list of enhancements from original `hybrid_chat.py`
- **LLM_OPTIMIZATION.md** - Details on response caching, dynamic tokens, and streaming
- **TIMING_TRACKING.md** - Response time tracking and performance monitoring
- **PHASE1_SIMPLIFICATION.md** - Query understanding approach
- **hybrid_chat.py** - Original version for reference

## ğŸ§ª Testing

```bash
# Test LLM optimizations (caching, tokens, streaming)
python test_llm_optimization.py

# Test timing tracking
python test_timing.py

# Test simplified Phase 1
python test_simplified_phase1.py
```

## ğŸ§¹ Cleanup (Optional)

To remove test files and redundant documentation:
```bash
# Run the cleanup script
.\cleanup.ps1
```

This keeps only essential files: README, IMPROVEMENTS, source code, and data.

## ğŸ¤ Support

This is an internship assignment project demonstrating hybrid RAG architecture with intelligent query understanding.
